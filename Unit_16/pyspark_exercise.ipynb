{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce using SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [SPARK](#SPARK)\n",
    "    * Installing Spark locally\n",
    "* [Spark Context](#Spark-Context)\n",
    "    * [Create A RDD](#Create-A-RDD)\n",
    "    * [Call `collect` on an RDD: Lazy Spark](#Call-collect-on-an-RDD:-Lazy-Spark)\n",
    "    * [Operations on RDDs](#Operations-on-RDDs)\n",
    "    * [Word Examples](#Word-Examples)\n",
    "    * [Key Value Pairs](#Key-Value-Pairs)\n",
    "    * [word count 1](#word-count-1)\n",
    "    * [word count 2:  `reduceByKey()`](#word-count-2:--reduceByKey%28%29)\n",
    "    * [Nested Syntax](#Nested-Syntax)\n",
    "    * [Using Cache](#Using-Cache)\n",
    "    * [Fun with words](#Fun-with-words)\n",
    "    * [DataFrames](#DataFrames)\n",
    "    * [Machine Learning](#Machine-Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With shameless stealing of some code and text from:\n",
    "\n",
    "- https://github.com/tdhopper/rta-pyspark-presentation/blob/master/slides.ipynb\n",
    "- Databricks and Berkeley Spark MOOC: https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x\n",
    "\n",
    "which you should go check out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Spark locally\n",
    "\n",
    "\n",
    "**Step 1: Install Apache Spark**\n",
    "\n",
    "For example, for Mac users using Homebrew:\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Install the Java SDK version 1.8 or above for your platform (not just the JRE runtime)**\n",
    "\n",
    "Make sure you can access commands such as `java` on your command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Install the latest findspark package using pip**\n",
    "\n",
    "```\n",
    "➜  ~  pip install findspark\n",
    "Collecting findspark\n",
    "  Downloading findspark-0.0.5-py2.py3-none-any.whl\n",
    "Installing collected packages: findspark\n",
    "Successfully installed findspark-0.0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Context\n",
    "\n",
    "You can also use it directly from the notebook interface on the mac if you installed `apache-spark` using `brew` and also installed `findspark` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also output's a bunch of stuff on my terminal. This is because the entire java context is started up.\n",
    "\n",
    "```Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "15/10/21 14:46:15 INFO SparkContext: Running Spark version 1.4.0\n",
    "2015-10-21 14:46:15.774 java[30685:c003] Unable to load realm info from SCDynamicStore\n",
    "15/10/21 14:46:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "15/10/21 14:46:15 INFO SecurityManager: Changing view acls to: rahul\n",
    "15/10/21 14:46:15 INFO SecurityManager: Changing modify acls to: rahul\n",
    "15/10/21 14:46:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(rahul); users with modify permissions: Set(rahul)\n",
    "15/10/21 14:46:16 INFO Slf4jLogger: Slf4jLogger started\n",
    "15/10/21 14:46:16 INFO Remoting: Starting remoting\n",
    "15/10/21 14:46:16 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.251.101.163:64359]\n",
    "15/10/21 14:46:16 INFO Utils: Successfully started service 'sparkDriver' on port 64359.\n",
    "15/10/21 14:46:16 INFO SparkEnv: Registering MapOutputTracker\n",
    "15/10/21 14:46:16 INFO SparkEnv: Registering BlockManagerMaster\n",
    "15/10/21 14:46:16 INFO DiskBlockManager: Created local directory at /private/var/folders/_f/y76rs29s3c57ykwyz9c8z12c0000gn/T/spark-00a4e09e-e5db-485f-81dc-2e5016e9a27e/blockmgr-8966e07c-223b-4c38-9273-11543aa9d3c1\n",
    "15/10/21 14:46:16 INFO MemoryStore: MemoryStore started with capacity 273.0 MB\n",
    "15/10/21 14:46:16 INFO HttpFileServer: HTTP File server directory is /private/var/folders/_f/y76rs29s3c57ykwyz9c8z12c0000gn/T/spark-00a4e09e-e5db-485f-81dc-2e5016e9a27e/httpd-6af0a9e0-1cfe-42c4-a1bd-e01715b98436\n",
    "15/10/21 14:46:16 INFO HttpServer: Starting HTTP Server\n",
    "15/10/21 14:46:17 INFO Utils: Successfully started service 'HTTP file server' on port 64360.\n",
    "15/10/21 14:46:17 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "15/10/21 14:46:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
    "15/10/21 14:46:18 INFO SparkUI: Started SparkUI at http://10.251.101.163:4040\n",
    "15/10/21 14:46:18 INFO Executor: Starting executor ID driver on host localhost\n",
    "15/10/21 14:46:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64361.\n",
    "15/10/21 14:46:18 INFO NettyBlockTransferService: Server created on 64361\n",
    "15/10/21 14:46:18 INFO BlockManagerMaster: Trying to register BlockManager\n",
    "15/10/21 14:46:18 INFO BlockManagerMasterEndpoint: Registering block manager localhost:64361 with 273.0 MB RAM, BlockManagerId(driver, localhost, 64361)\n",
    "15/10/21 14:46:18 INFO BlockManagerMaster: Registered BlockManager\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-RMO32R2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).map(lambda x: x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create A RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "type(wordsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Call `collect` on an RDD: Lazy Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is lazy. Until you `collect`, nothing is actually run.\n",
    "\n",
    ">Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'elephant', 'rat', 'rat', 'cat']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "15/10/21 14:59:59 INFO SparkContext: Starting job: collect at <ipython-input-6-dee494da0714>:1\n",
    "15/10/21 14:59:59 INFO DAGScheduler: Got job 0 (collect at <ipython-input-6-dee494da0714>:1) with 4 output partitions (allowLocal=false)\n",
    "15/10/21 14:59:59 INFO DAGScheduler: Final stage: ResultStage 0(collect at <ipython-input-6-dee494da0714>:1)\n",
    "15/10/21 14:59:59 INFO DAGScheduler: Parents of final stage: List()\n",
    "15/10/21 14:59:59 INFO DAGScheduler: Missing parents: List()\n",
    "15/10/21 14:59:59 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:396), which has no missing parents\n",
    "15/10/21 15:00:00 INFO MemoryStore: ensureFreeSpace(1224) called with curMem=0, maxMem=286300569\n",
    "15/10/21 15:00:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1224.0 B, free 273.0 MB)\n",
    "15/10/21 15:00:00 INFO MemoryStore: ensureFreeSpace(777) called with curMem=1224, maxMem=286300569\n",
    "15/10/21 15:00:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 777.0 B, free 273.0 MB)\n",
    "15/10/21 15:00:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:64361 (size: 777.0 B, free: 273.0 MB)\n",
    "15/10/21 15:00:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874\n",
    "15/10/21 15:00:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:396)\n",
    "15/10/21 15:00:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1379 bytes)\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1384 bytes)\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1379 bytes)\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1403 bytes)\n",
    "15/10/21 15:00:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
    "15/10/21 15:00:00 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
    "15/10/21 15:00:00 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
    "15/10/21 15:00:00 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
    "15/10/21 15:00:00 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 646 bytes result sent to driver\n",
    "15/10/21 15:00:00 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 665 bytes result sent to driver\n",
    "15/10/21 15:00:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 641 bytes result sent to driver\n",
    "15/10/21 15:00:00 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 641 bytes result sent to driver\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 61 ms on localhost (1/4)\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 60 ms on localhost (2/4)\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 96 ms on localhost (3/4)\n",
    "15/10/21 15:00:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 63 ms on localhost (4/4)\n",
    "15/10/21 15:00:00 INFO DAGScheduler: ResultStage 0 (collect at <ipython-input-6-dee494da0714>:1) finished in 0.120 s\n",
    "15/10/21 15:00:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\n",
    "15/10/21 15:00:00 INFO DAGScheduler: Job 0 finished: collect at <ipython-input-6-dee494da0714>:1, took 0.872367 s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Spark Programming Guide:\n",
    "\n",
    ">RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cats'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "\n",
    "makePlural('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform one RDD into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "['cats', 'elephants']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "print(pluralRDD.first())\n",
    "print(pluralRDD.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluralRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'elephants', 'rats', 'rats', 'cats']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda w: (w, 1))\n",
    "wordPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "➜  sparklect  ps auxwww | grep pyspark\n",
    "rahul           30685   0.4  0.8  3458120  68712 s012  S+    2:46PM   2:00.21 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java -cp /usr/local/opt/apache-spark/libexec/conf/:/usr/local/opt/apache-spark/libexec/lib/spark-assembly-1.4.0-hadoop2.6.0.jar:/usr/local/opt/apache-spark/libexec/lib/datanucleus-api-jdo-3.2.6.jar:/usr/local/opt/apache-spark/libexec/lib/datanucleus-core-3.2.10.jar:/usr/local/opt/apache-spark/libexec/lib/datanucleus-rdbms-3.2.9.jar -Xms512m -Xmx512m -XX:MaxPermSize=128m org.apache.spark.deploy.SparkSubmit pyspark-shell\n",
    "rahul           31520   0.0  0.0  2432784    480 s011  R+    6:42PM   0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=.cvs --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn pyspark\n",
    "rahul           31494   0.0  0.7  2548972  57288 s012  S     6:41PM   0:00.10 python -m pyspark.daemon\n",
    "rahul           31493   0.0  0.7  2548972  57308 s012  S     6:41PM   0:00.10 python -m pyspark.daemon\n",
    "rahul           31492   0.0  0.7  2548972  57288 s012  S     6:41PM   0:00.11 python -m pyspark.daemon\n",
    "rahul           31446   0.0  0.8  2548972  68460 s012  S     6:35PM   0:01.34 python -m pyspark.daemon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### WORD COUNT!\n",
    "\n",
    "This little exercise shows how to use mapreduce to calculate the counts of individual words in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 2), ('elephant', 1), ('rat', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda w: (w, 1))\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                       .collect())\n",
    "wordCountsCollected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tons of shuffling](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/images/reduce_by.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[24] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'toDebugString'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-900ea1b565e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordsRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'toDebugString'"
     ]
    }
   ],
   "source": [
    "print(wordsRDD.map(lambda w: (w, 1)).reduceByKey(lambda x,y: x+y)).toDebugString()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[25] at readRDDFromFile at PythonRDD.scala:247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "print(wordsRDD)\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, every operation is run from the start. This may be inefficient in many cases. So when appropriate, we may want to cache the result the first time an operation is run on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is rerun from the start\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[25] at readRDDFromFile at PythonRDD.scala:247"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default storage level (MEMORY_ONLY)\n",
    "wordsRDD.cache()#nothing done this is still lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parallelize is rerun and cached because we told it to cache\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this `sc.parallelize` is not rerun in this case\n",
    "wordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is this useful: it is when you have branching parts or loops, so that you dont do things again and again. Spark, being \"lazy\" will rerun the chain again. So `cache` or `persist` serves as a checkpoint, breaking the RDD chain or the *lineage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 'mammal',\n",
       " 'elephant': 'mammal',\n",
       " 'rat': 'mammal',\n",
       " 'heron': 'bird',\n",
       " 'owl': 'bird'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birdsList=['heron','owl']\n",
    "animList=wordsList+birdsList\n",
    "animaldict={}\n",
    "for e in wordsList:\n",
    "    animaldict[e]='mammal'\n",
    "for e in birdsList:\n",
    "    animaldict[e]='bird'\n",
    "animaldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2\n"
     ]
    }
   ],
   "source": [
    "animsrdd = sc.parallelize(animList, 4)\n",
    "animsrdd.cache()\n",
    "#below runs the whole chain but causes cache to be populated\n",
    "mammalcount=animsrdd.filter(lambda w: animaldict[w]=='mammal').count()\n",
    "#now only the filter is carried out\n",
    "birdcount=animsrdd.filter(lambda w: animaldict[w]=='bird').count()\n",
    "print(mammalcount, birdcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: Fun with MapReduce\n",
    "\n",
    "Read http://spark.apache.org/docs/latest/programming-guide.html for some useful background and then try out the following exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `./sparklect/english.stop.txt` contains a list of English stopwords, while the file `./sparklect/shakes/juliuscaesar.txt` contains the entire text of Shakespeare's 'Julius Caesar'.\n",
    "\n",
    "* Load all of the stopwords into a Python list\n",
    "* Load the text of Julius Caesar into an RDD using the `sparkcontext.textfile()` method. Call it `juliusrdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn\n",
    "stopwords=[e.strip() for e in open(\"./sparklect/english.stop.txt\").readlines()]\n",
    "juliusrdd=sc.textFile(\"./sparklect/shakes/juliuscaesar.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words does Julius Caesar have? *Hint: use `flatMap()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21245"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your turn\n",
    "juliusrdd.flatMap(lambda line: line.split()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 20 words of Julius Caesar as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1599',\n",
       " 'the',\n",
       " 'tragedy',\n",
       " 'of',\n",
       " 'julius',\n",
       " 'caesar',\n",
       " 'by',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'dramatis',\n",
       " 'personae',\n",
       " 'julius',\n",
       " 'caesar,',\n",
       " 'roman',\n",
       " 'statesman',\n",
       " 'and',\n",
       " 'general',\n",
       " 'octavius,',\n",
       " 'triumvir',\n",
       " 'after']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your turn\n",
    "(juliusrdd.flatMap(lambda line: line.split())\n",
    " .map(lambda word: word.strip().lower())\n",
    "  .take(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 20 words of Julius Caesar, **after removing all the stopwords**. *Hint: use `filter()`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1599',\n",
       " 'tragedy',\n",
       " 'julius',\n",
       " 'caesar',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " 'dramatis',\n",
       " 'personae',\n",
       " 'julius',\n",
       " 'caesar,',\n",
       " 'roman',\n",
       " 'statesman',\n",
       " 'general',\n",
       " 'octavius,',\n",
       " 'triumvir',\n",
       " \"caesar's\",\n",
       " 'death,',\n",
       " 'augustus',\n",
       " 'caesar,',\n",
       " 'emperor']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your turn\n",
    "(juliusrdd.flatMap(lambda line: line.split())\n",
    " .map(lambda word: word.strip().lower())\n",
    " .filter(lambda word: word not in stopwords)\n",
    " .take(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the word counting MapReduce code you've seen before. Count the number of times each word occurs and print the top 20 results as a list of tuples of the form `(word, count)`. *Hint: use `takeOrdered()` instead of `take()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brutus.', 211),\n",
       " ('cassius.', 152),\n",
       " ('thou', 107),\n",
       " ('caesar', 96),\n",
       " ('brutus', 75),\n",
       " ('antony.', 73),\n",
       " ('citizen.', 68),\n",
       " ('good', 66),\n",
       " ('caesar.', 62),\n",
       " ('thy', 54),\n",
       " ('brutus,', 54),\n",
       " ('caesar,', 46),\n",
       " ('\"', 44),\n",
       " ('casca.', 44),\n",
       " ('you,', 41),\n",
       " ('men', 41),\n",
       " (\"caesar's\", 40),\n",
       " ('enter', 40),\n",
       " ('lucius.', 38),\n",
       " ('cassius,', 38)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your turn\n",
    "(juliusrdd.flatMap(lambda line: line.split())\n",
    " .map(lambda word: word.strip().lower())\n",
    " .filter(lambda word: word not in stopwords)\n",
    " .map(lambda word: (word, 1))\n",
    " .reduceByKey(lambda a, b: a + b)\n",
    " .takeOrdered(20, lambda x: -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a bar graph. For each of the top 20 words on the X axis, represent the count on the Y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your turn\n",
    "captions, counts=zip(*juliusrdd.flatMap(lambda line: line.split())\n",
    " .map(lambda word: word.strip().lower())\n",
    " .filter(lambda word: word not in stopwords)\n",
    " .map(lambda word: (word, 1))\n",
    " .reduceByKey(lambda a, b: a + b)\n",
    " .takeOrdered(20, lambda x: -x[1])\n",
    ")\n",
    "pos = np.arange(len(counts))\n",
    "plt.bar(pos, counts);\n",
    "plt.xticks(pos+0.4, captions, rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=list(zip(*juliusrdd.flatMap(lambda line: line.split())\n",
    " .map(lambda word: word.strip().lower())\n",
    " .filter(lambda word: word not in stopwords)\n",
    " .map(lambda word: (word, 1))\n",
    " .reduceByKey(lambda a, b: a + b)\n",
    " .take(1000))\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20547d0c4e0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(counts,bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using partitions for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make your code more efficient, you want to use all of the available processing power, even on a single laptop. If your machine has multiple cores, you can tune the number of partitions to use all of them! From http://www.stat.berkeley.edu/scf/paciorek-spark-2014.html:\n",
    "\n",
    ">You want each partition to be able to fit in the memory availalbe on a node, and if you have multi-core nodes, you want that as many partitions as there are cores be able to fit in memory.\n",
    "\n",
    ">For load-balancing you'll want at least as many partitions as total computational cores in your cluster and probably rather more partitions. The Spark documentation suggests 2-4 partitions (which they also seem to call slices) per CPU. Often there are 100-10,000 partitions. Another rule of thumb is that tasks should take at least 100 ms. If less than that, you may want to repartition to have fewer partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakesrdd=sc.textFile(\"./sparklect/shakes/*.txt\", minPartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1350.partitions.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)\r\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2034)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:269)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-7a7859d577a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshakesrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1398\u001b[0m         \"\"\"\n\u001b[0;32m   1399\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1400\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1401\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1350.partitions.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)\r\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2034)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:269)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "shakesrdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the top 20 words in all of the files that you just read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o578.partitions.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)\r\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2034)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:269)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-00f0d94ac8b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m  \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m  \u001b[1;33m.\u001b[0m\u001b[0mtakeOrdered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[1;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[0;32m   1696\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \"\"\"\n\u001b[1;32m-> 1698\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[1;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \"\"\"\n\u001b[0;32m   1924\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1925\u001b[1;33m             \u001b[0mnumPartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2333\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2600\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2601\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o578.partitions.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:77)\r\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:235)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:149)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2034)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:269)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:272)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# your turn\n",
    "(shakesrdd.flatMap(lambda line: line.split())\n",
    " .map(lambda word: word.strip().lower())\n",
    " .filter(lambda word: word not in stopwords)\n",
    " .map(lambda word: (word, 1))\n",
    " .reduceByKey(lambda a, b: a + b)\n",
    " .takeOrdered(20,lambda x: -x[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional topic 1: DataFrames\n",
    "\n",
    "Pandas and Spark dataframes can be easily converted to each other, making it easier to work with different data formats. This section shows some examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Spark DataFrame to Pandas\n",
    "\n",
    "`pandas_df = spark_df.toPandas()`\n",
    "\n",
    "Create a Spark DataFrame from Pandas\n",
    "\n",
    "`spark_df = context.createDataFrame(pandas_df)`\n",
    "\n",
    "Must fit in memory.\n",
    "\n",
    "![](https://ogirardot.files.wordpress.com/2015/05/rdd-vs-dataframe.png?w=640&h=360)\n",
    "\n",
    "VERY IMPORTANT: DataFrames in Spark are like RDD in the sense that they’re an immutable data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>73.847017</td>\n",
       "      <td>241.893563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>68.781904</td>\n",
       "      <td>162.310473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>74.110105</td>\n",
       "      <td>212.740856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>71.730978</td>\n",
       "      <td>220.042470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>69.881796</td>\n",
       "      <td>206.349801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender     Height      Weight\n",
       "0   Male  73.847017  241.893563\n",
       "1   Male  68.781904  162.310473\n",
       "2   Male  74.110105  212.740856\n",
       "3   Male  71.730978  220.042470\n",
       "4   Male  69.881796  206.349801"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"sparklect/01_heights_weights_genders.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this pandas dataframe to a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Gender: string, Height: double, Weight: double]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)\n",
    "sparkdf = sqlsc.createDataFrame(df)\n",
    "sparkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+\n",
      "|Gender|           Height|          Weight|\n",
      "+------+-----------------+----------------+\n",
      "|  Male|  73.847017017515|241.893563180437|\n",
      "|  Male|68.78190404589029|  162.3104725213|\n",
      "|  Male|74.11010539178491|  212.7408555565|\n",
      "|  Male| 71.7309784033377|220.042470303077|\n",
      "|  Male| 69.8817958611153|206.349800623871|\n",
      "+------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparkdf.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male',\n",
       " 'Male']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can't call .map() on a DataFrame directly - you first have to convert it into an RDD\n",
    "temp = sparkdf.rdd.map(lambda r: r.Gender)\n",
    "print(type(temp))\n",
    "temp.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional topic 2: Machine Learning using Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a data set from the Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [73.847017017515,241.893563180437]),\n",
       " LabeledPoint(1.0, [68.78190404589029,162.3104725213]),\n",
       " LabeledPoint(1.0, [74.11010539178491,212.7408555565]),\n",
       " LabeledPoint(1.0, [71.7309784033377,220.042470303077]),\n",
       " LabeledPoint(1.0, [69.8817958611153,206.349800623871])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=sparkdf.rdd.map(lambda row: LabeledPoint(row.Gender=='Male',[row.Height, row.Weight]))\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, DenseVector([73.847, 241.8936]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=sparkdf.rdd.map(lambda row: LabeledPoint(row[0]=='Male',row[1:]))\n",
    "data2.take(1)[0].label, data2.take(1)[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[119] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = data.randomSplit([0.7,0.3])\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the logistic regression model using MLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.4819, 0.1981])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 1.0), (1.0, 1.0), (1.0, 0.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 0.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = test.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n",
    "print(results.take(10))\n",
    "type(results)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure accuracy and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 118.0 failed 1 times, most recent failure: Lost task 1.0 in stage 118.0 (TID 348, DESKTOP-RMO32R2, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 598, in main\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in process\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'p'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2135)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2160)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 598, in main\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in process\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'p'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2135)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-deefec7a7cc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \"\"\"\n\u001b[1;32m-> 1128\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[1;36m6.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m         \"\"\"\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;31m# to the final reduce call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 118.0 failed 1 times, most recent failure: Lost task 1.0 in stage 118.0 (TID 348, DESKTOP-RMO32R2, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 598, in main\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in process\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'p'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2135)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2160)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 598, in main\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 588, in process\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 2583, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 425, in func\n    return f(iterator)\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1128, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"C:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 105, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'p'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2135)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=results.filter(lambda a,p: a==p).count()/float(results.count())\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.evaluation.BinaryClassificationMetrics'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9141875306823759"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(metrics))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mylogistic.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o983.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.mllib.classification.impl.GLMClassificationModel$SaveLoadV1_0$.save(GLMClassificationModel.scala:60)\r\n\tat org.apache.spark.mllib.classification.LogisticRegressionModel.save(LogisticRegression.scala:163)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 112.0 failed 1 times, most recent failure: Lost task 0.0 in stage 112.0 (TID 339, DESKTOP-RMO32R2, executor driver): org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:484)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:172)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitTask(OutputCommitter.java:343)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:245)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:140)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:129)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 49 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:484)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:172)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitTask(OutputCommitter.java:343)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:245)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:140)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:129)\r\n\t... 9 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-d93a72868b8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mylogistic.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sc, path)\u001b[0m\n\u001b[0;32m    238\u001b[0m         java_model = sc._jvm.org.apache.spark.mllib.classification.LogisticRegressionModel(\n\u001b[0;32m    239\u001b[0m             _py2java(sc, self._coeff), self.intercept, self.numFeatures, self.numClasses)\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0mjava_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.0.0-preview-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o983.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.mllib.classification.impl.GLMClassificationModel$SaveLoadV1_0$.save(GLMClassificationModel.scala:60)\r\n\tat org.apache.spark.mllib.classification.LogisticRegressionModel.save(LogisticRegression.scala:163)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 112.0 failed 1 times, most recent failure: Lost task 0.0 in stage 112.0 (TID 339, DESKTOP-RMO32R2, executor driver): org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:484)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:172)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitTask(OutputCommitter.java:343)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:245)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:140)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:129)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1979)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1967)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1966)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:946)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:946)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2196)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2145)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2134)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:748)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2095)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 49 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:157)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:83)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:484)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:597)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:172)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitTask(OutputCommitter.java:343)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)\r\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:245)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:140)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:129)\r\n\t... 9 more\r\n"
     ]
    }
   ],
   "source": [
    "model.save(sc, \"mylogistic.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline API automates a lot of this stuff, allowing us to work directly on dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see:\n",
    "\n",
    "- http://jordicasanellas.weebly.com/data-science-blog/machine-learning-with-spark\n",
    "- http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "- http://www.techpoweredmath.com/spark-dataframes-mllib-tutorial/\n",
    "- http://spark.apache.org/docs/latest/api/python/\n",
    "- http://spark.apache.org/docs/latest/programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdd.saveAsTextFile()` saves an RDD as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Topic 3: Your Turn at Machine Learning! :)\n",
    "\n",
    "For this exercise, we're going to use one of the datasets we've already worked with: the Boston House Prices dataset. We're going to try a couple of regression algorithms, but from the SparkML library this time.\n",
    "\n",
    "Before you proceed, make sure to do an overview of the documentation: \n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.ml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to load the dataset, which resides as a CSV file in the folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path: /sparklect/boston.csv\n",
    "data = spark.read.csv(\"./sparklect/boston.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data to make sure everything is loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio| black|lstat|medv|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
      "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|\n",
      "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
      "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|\n",
      "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|16.5|\n",
      "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|\n",
      "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|392.52|20.45|15.0|\n",
      "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|\n",
      "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2| 390.5|15.71|21.7|\n",
      "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26|20.4|\n",
      "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|380.02|10.26|18.2|\n",
      "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0|395.62| 8.47|19.9|\n",
      "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307|   21.0|386.85| 6.58|23.1|\n",
      "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|386.75|14.67|17.5|\n",
      "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|288.99|11.69|20.2|\n",
      "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|390.95|11.28|18.2|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to create a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-ab7688bbc611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'final_data' is not defined"
     ]
    }
   ],
   "source": [
    "# We'll first have to vectorize the features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"crim\", \"zn\", \"indus\", \"chas\", \"nox\", \"rm\", \"age\", \"dis\", \"rad\", \"tax\", \"ptratio\", \"black\", \"lstat\"],\n",
    "    outputCols=\"features\")\n",
    "\n",
    "output = assembler.transform(data)\n",
    "\n",
    "final_data = output.select(\"features\", \"medv\")\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, fit a Linear Regression model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-80f30851f880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"medv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlrModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"medv\")\n",
    "lrModel = lr.fit(train_data,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now validate the model on the test set, and check the Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-38426fc435bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munlabeled_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrootMeanSquaredError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lrModel' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = lrModel.evaluate(test_data)\n",
    "unlabeled_data = test_data.select(\"features\")\n",
    "predictions = lrModel.transform(unlabeled_data)\n",
    "print(\"RMSE: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Linear Regression with a more powerful algorithm - the Random Forest. As the Random Forest has several hyperparameters that can be tuned for maximum accuracy, we're going to need to use k-fold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up a grid for the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(labelCol=\"medv\", featuresCol=\"features\")\n",
    "grid = ParamGridBuilder().addGrid(rf.numTrees, [100, 250, 500]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with a Random Forest regressor using k-fold Cross Validation, and find the optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-b3729faec507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m crossval = CrossValidator(\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mestimatorParamMaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mevaluator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5)\n",
    "\n",
    "rfModel = cv.fit(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate the model on the test set and check the Root Mean Squared Error again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rfModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-63805c7bb260>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_test_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrootMeanSquaredError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rfModel' is not defined"
     ]
    }
   ],
   "source": [
    "cv_test_results = rfModel.evaluate(test_data)\n",
    "print(\"RMSE: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Optional Topic 4: Model Your Capstone Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time, load up the cleaned dataset from one of your capstone projects. Do you remember which algorithm and the accompanying combination of hyperparameters did the best job? For practice, try and implement the same model in SparkML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
